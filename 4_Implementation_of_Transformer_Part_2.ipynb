{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlIWlCu1cb5y"
      },
      "source": [
        "# Programming Assingnment4 - Implementation of Transformer Part 2\n",
        "\n",
        "안녕하세요, **AIKU 학회원 여러분**. 네 번째 과제에서는 지난 주에 작업했던 Transformer Encoder를 바탕으로 Transformer 전체 구조를 완성하고, \"Attention is All You Need\" 논문에서 다루었던 기계 번역(machine translation) 작업을 수행하게 됩니다. Encoder 부분은 이미 구현되어 있으므로 별도의 구현이 필요하지 않습니다. 지난주에 구현한 **2-4 Encoder Layer**에 이어, 이번 주에는 **2-5 Decoder Layer**부터 구현하시면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka6H2paBcm85"
      },
      "source": [
        "![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/transformer-model-architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wm7BGAYLcy5d"
      },
      "source": [
        "## Part 1. 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lS-c6WnCc0Bn"
      },
      "source": [
        "### 1-1. Pip install & Imports\n",
        "필요한 패키지를 pip를 이용해서 설치합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nps-lwDmZA1y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install torchdata torchtext spacy portalocker matplotlib seaborn\n",
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmvfJSEudBmZ"
      },
      "source": [
        "필요한 라이브러리를 import하고, device를 GPU로 바꿔줍니다.\n",
        "\n",
        "다만, 코드를 구현하는 과정에서는 **CPU 사용**을 권장드립니다.\n",
        "\n",
        "GPU가 가장 많이 필요한 단계는 학습하는 과정인데 구현하는 시간동안 colab의 GPU를 다 써버리면 막상 학습할 때 필요한 GPU 자원을 쓸 수 없게 됩니다. 따라서 코드를 모두 구현하고 train 코드가 잘 돌아가는지 확인한 뒤에 colab 상단 런타임 메뉴에서 런타임 유형을 GPU로 바꾼 뒤에 실행하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YZsPcGDW-mj"
      },
      "outputs": [],
      "source": [
        "# Description: Transformer Model in PyTorch\n",
        "\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import TensorDataset, DataLoader, dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('using device: ', device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWw6GyMX7BSi"
      },
      "source": [
        "## Part 2. Transformer Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsw_H4nPyu0d"
      },
      "source": [
        "Transformer에 대해서 배우기 전에 기존의 seq2seq를 상기해봅시다. 기존의 seq2seq 모델은 인코더-디코더 구조로 구성되어져 있었습니다. 여기서 인코더는 입력 시퀀스를 하나의 벡터 표현으로 압축하고, 디코더는 이 벡터 표현을 통해서 출력 시퀀스를 만들어냈습니다. 하지만 이러한 구조는 인코더가 입력 시퀀스를 하나의 벡터로 압축하는 과정에서 입력 시퀀스의 정보가 일부 손실된다는 단점이 있었고, 이를 보정하기 위해 어텐션이 사용되었습니다. 그런데 어텐션을 RNN의 보정을 위한 용도로서 사용하는 것이 아니라 어텐션만으로 인코더와 디코더를 만들어보면 어떨까요?\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/transformer1.PNG)\n",
        "\n",
        "Transformer는 RNN을 사용하지 않지만 기존의 seq2seq처럼 인코더에서 입력 시퀀스를 입력받고, 디코더에서 출력 시퀀스를 출력하는 인코더-디코더 구조를 유지하고 있습니다. 이전 seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time step)을 가지는 구조였다면 이번에는 인코더와 디코더라는 단위가 N개로 구성되는 구조입니다. Transformer를 제안한 논문에서는 인코더와 디코더의 개수를 각각 6개 사용하였습니다.\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/transformer2.PNG)\n",
        "\n",
        "위의 그림은 인코더와 디코더가 6개씩 존재하는 Transformer의 구조를 보여줍니다. 이 책에서는 인코더와 디코더가 각각 여러 개 쌓여있다는 의미를 사용할 때는 알파벳 s를 뒤에 붙여 encoders, decoders라고 표현하겠습니다.\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/transformer4_final_final_final.PNG)\n",
        "\n",
        "위의 그림은 인코더로부터 정보를 전달받아 디코더가 출력 결과를 만들어내는 트랜스포머 구조를 보여줍니다. 디코더는 마치 기존의 seq2seq 구조처럼 시작 심볼 <sos>를 입력으로 받아 종료 심볼 <eos>가 나올 때까지 연산을 진행합니다. 이는 RNN은 사용되지 않지만 여전히 인코더-디코더의 구조는 유지되고 있음을 보여줍니다.\n",
        "\n",
        "Transformer의 내부 구조를 조금씩 확대해가는 방식으로 Transformer를 이해해봅시다. 우선 인코더와 디코더의 구조를 이해하기 전에 Transformer의 입력에 대해서 이해해보겠습니다. Transformer의 인코더와 디코더는 단순히 각 단어의 임베딩 벡터들을 입력받는 것이 아니라 임베딩 벡터에서 조정된 값을 입력받는데 이에 대해서 알아보기 위해 입력 부분을 확대해보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feQfNhVIiYJq"
      },
      "source": [
        "## 2-1. Transformer Embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVBa9llx7PQU"
      },
      "source": [
        "Transformer의 내부를 이해하기 전 우선 **Transformer의 입력**에 대해서 알아보겠습니다. RNN이 자연어 처리에서 유용했던 이유는 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해 각 단어의 위치 정보(position information)를 가질 수 있다는 점에 있었습니다.\n",
        "\n",
        "하지만 Transformer는 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줄 필요가 있습니다. Transformer는 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하는데, 이를 **포지셔널 인코딩(positional encoding)**이라고 합니다.\n",
        "\n",
        "Positional Encoding은 입력 문장 단어(토큰)들에 대한 위치정보를 인코딩하는 기술로, Transformer 모델의 입력 임베딩에 이 정보를 추가함으로써 단어의 상대적인 위치를 반영할 수 있습니다. 이렇게 함으로써 모델은 문장의 구조와 순서를 학습할 수 있게 됩니다.\n",
        "\n",
        "다음 코드는 Transformer에서 사용되는 대표적인 Positional Encoding 방법 중 하나인 Sinusodial Positional Encoding입니다. 이 방법은 고정된 함수로서 주기적인 값을 부여하여 위치정보를 인코딩합니다.\n",
        "\n",
        "Sinusodial Positional Encoding은 다음과 같은 수식을 사용하여 위치 인코딩 값을 계산합니다.\n",
        "\n",
        "$PE_{(pos,2i)}​ =sin(\\frac {pos}{10000^ {2i/d} model​}​ )$\n",
        "\n",
        "$PE_{(pos,2i+1)}​ =cos(\\frac {pos}{10000^ {2i/d} model​}​ )$\n",
        "\n",
        "여기서 $PE_{(pos, 2i)}$와 $PE_{(pos, 2i+1)}$는 Positional Encoding 행렬에서 $(pos, 2i)$와 $(pos, 2i+1)$ 위치에 해당하는 값을 의미하며, $pos$는 단어의 위치(position)를 나타내고, $i$는 인코딩 차원의 인덱스를 의미합니다. $d_{\\text{model}}$은 임베딩 차원의 크기를 나타냅니다.\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/transformer7.PNG)\n",
        "\n",
        "$pos$는 입력 문장에서의 임베딩 벡터의 위치를 나타내며, $i$는 임베딩 벡터 내의 차원의 인덱스를 의미합니다. 위의 식에 따르면 임베딩 벡터 내의 각 차원의 인덱스가 짝수인 경우에는 사인 함수의 값을 사용하고 홀수인 경우에는 코사인 함수의 값을 사용합니다. 위의 수식에서 $(pos, 2i)$일 때는 사인 함수를 사용하고,  $(pos, 2i+1)$ 일 때는 코사인 함수를 사용하고 있음을 주목합시다.\n",
        "\n",
        "\n",
        "이렇게 구해진 Positional Encoding 행렬은 입력 임베딩과 더해져서 최종 입력으로 들어가게 되며, 모델은 이를 활용하여 문장의 구조와 순서를 이해하고 학습합니다.\n",
        "\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/transformer5_final_final.PNG)\n",
        "\n",
        "위의 그림은 입력으로 사용되는 임베딩 벡터들이 트랜스포머의 입력으로 사용되기 전에 포지셔널 인코딩의 값이 더해지는 것을 보여줍니다. 임베딩 벡터가 인코더의 입력으로 사용되기 전 포지셔널 인코딩값이 더해지는 과정을 시각화하면 아래와 같습니다.\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/transformer6_final.PNG)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1sNGC1l0ye5"
      },
      "outputs": [],
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeddings = self.embedding(x)\n",
        "        embeddings = embeddings.to(x.device)\n",
        "\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DubXWXYZ0y8F"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_length, embedding_dim):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.positional_encoding = self.create_positional_encoding(max_length, embedding_dim)\n",
        "\n",
        "    def get_angles(self, max_length, embedding_dim):\n",
        "        \"\"\"\n",
        "        Calculates the angle values for positional encoding in a transformer model.\n",
        "\n",
        "        This function computes the sinusoidal positional encoding angles based on the specified maximum sequence length and the dimensionality of the embeddings.\n",
        "        The positional encodings are used to add some information about the relative or absolute position of the tokens in the sequence.\n",
        "        The formula used here ensures that each dimension of the positional encoding corresponds to a sinusoid of different frequencies and phases.\n",
        "\n",
        "        Parameters:\n",
        "        max_length (int): The maximum length of the input sequences for which the positional encodings are to be generated.\n",
        "        embedding_dim (int): The dimensionality of the embeddings. The number of dimensions should be even as the sinusoids are created for half this dimension size.\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: A tensor of shape (max_length, embedding_dim // 2) containing the positional encoding angles.\n",
        "        \"\"\"\n",
        "\n",
        "        position = torch.arange(max_length).unsqueeze(1)\n",
        "        div_term = torch.pow(10000, torch.arange(0, embedding_dim, 2).float() / embedding_dim)\n",
        "\n",
        "        return position / div_term  # (max_length, embedding_dim / 2)\n",
        "\n",
        "    def create_positional_encoding(self, max_length, embedding_dim):\n",
        "        \"\"\"\n",
        "        Generates the positional encoding matrix for transformer models.\n",
        "        This function creates a positional encoding matrix using sinusoidal functions.\n",
        "        These encodings provide the model with information about the position of the tokens in the sequence.\n",
        "\n",
        "        Parameters:\n",
        "        max_length (int): The maximum length of the input sequences for which the positional encodings are to be generated.\n",
        "        embedding_dim (int): The dimensionality of the embeddings.\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: A tensor of shape (max_length, embedding_dim) containing the positional encoding matrix. Each row represents the positional encoding of a token in the sequence at different positions.\n",
        "\n",
        "        Notes:\n",
        "        The encoding at each position is created by applying the sine function to even indices of the embedding dimensions and the cosine function to the odd indices.\n",
        "        \"\"\"\n",
        "\n",
        "        angles = self.get_angles(max_length, embedding_dim)\n",
        "        positional_encoding = torch.zeros(max_length, embedding_dim)\n",
        "\n",
        "        # torch.sin(angles)와 torch.cos(angles)를 차원에 맞게 positional_encoding에 할당\n",
        "        positional_encoding[:, 0::2] = torch.sin(angles)\n",
        "        positional_encoding[:, 1::2] = torch.cos(angles)\n",
        "\n",
        "        return positional_encoding\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Applies the positional encoding to the input tensor and returns the modified tensor.\n",
        "        This method adjusts the positional encoding to the device of the input tensor `x` and adds the positional encoding to `x`.\n",
        "        The positional encoding enhances the model's ability to understand the position of each element in the sequence. The addition of positional encodings to the input is a standard technique used in models like transformers to maintain the order of the input data.\n",
        "\n",
        "        Parameters:\n",
        "        x (torch.Tensor): The input tensor to which positional encodings need to be added.\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: The tensor resulting from adding positional encodings to the input tensor `x`. This output tensor has the same shape as the input tensor.\n",
        "\n",
        "        Notes:\n",
        "        The positional encodings are first transferred to the same device as `x` to ensure compatibility in operations.\n",
        "        Only the required portion of the positional encoding matrix is used, corresponding to the actual sequence length of `x`.\n",
        "        This portion is detached to prevent gradients from flowing into the positional encoding during backpropagation.\n",
        "        \"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        positional_encoding = self.positional_encoding.to(x.device)\n",
        "\n",
        "        return positional_encoding[:x.size(1), :].detach()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpAyuebqCpqB"
      },
      "source": [
        "제대로 구현했다면 다음과 같은 그래프가 생성되어야 합니다!\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/transformer8_final_ver.png)\n",
        "\n",
        "직접 구현한 PositionalEncoding class로 아래 코드를 이용해서 그래프를 생성해보세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dnhz5xI6Ci4o"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create an instance of the PositionalEncoding class\n",
        "max_length = 50\n",
        "embedding_dim = 128\n",
        "\n",
        "# Create an instance of the PositionalEncoding class\n",
        "pos_encoding = PositionalEncoding(max_length, embedding_dim)\n",
        "\n",
        "# Convert the positional encoding tensor to a numpy array\n",
        "pos_encoding_matrix = pos_encoding.positional_encoding.detach().numpy()\n",
        "\n",
        "# Plotting the heatmap of positional encodings\n",
        "plt.pcolormesh(pos_encoding_matrix, cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 128))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12kjCj2VhCCR"
      },
      "source": [
        "이제 전체 embedding값을 완성해봅시다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nj7yRCg4gUGa"
      },
      "outputs": [],
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Embedding layer for transformer models that combines token embeddings with positional encodings.\n",
        "\n",
        "    This class implements an embedding layer specific to transformer architectures, combining learned token embeddings with fixed (or learned) positional encodings. The token embeddings convert token indices into embeddings and the positional encodings provide additional context about the position of tokens within the sequence, which is crucial for models without recurrent structure.\n",
        "\n",
        "    Attributes:\n",
        "    tok_emb (TokenEmbedding): A module to convert token indices into embeddings.\n",
        "    pos_emb (PositionalEncoding): A module to generate positional encodings for tokens.\n",
        "\n",
        "    Parameters:\n",
        "    vocab_size (int): The size of the vocabulary.\n",
        "    embedding_dim (int): The dimensionality of the embeddings.\n",
        "    max_len (int): The maximum length of the input sequences. Note that `max_len` should match the maximum length expected in the positional encoding module.\n",
        "\n",
        "    Methods:\n",
        "    forward(x): Computes the embeddings by summing token and positional embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, max_len):\n",
        "\n",
        "        super(TransformerEmbedding, self).__init__()\n",
        "        self.tok_emb = TokenEmbedding(vocab_size, embedding_dim)\n",
        "        self.pos_emb = PositionalEncoding(max_len, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Computes the embeddings for input indices by summing token and positional embeddings.\n",
        "\n",
        "        Parameters:\n",
        "        x (torch.Tensor): The input tensor containing token indices.\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: The tensor containing combined embeddings which are the sum of token and positional embeddings.\n",
        "        \"\"\"\n",
        "\n",
        "        tok_emb = self.tok_emb(x)\n",
        "        pos_emb = self.pos_emb(x)\n",
        "\n",
        "        return tok_emb + pos_emb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA-x5zNoizau"
      },
      "source": [
        "## 2-2. Attention in Transformer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJxuSYxbi7no"
      },
      "source": [
        "### 2-2-1. Scaled Dot Product Attention\n",
        "\n",
        "![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/scale_dot_product_attention.png)\n",
        "\n",
        "Self-Attention 메커니즘 중 하나로, 입력 시퀀스 내의 단어들 간의 관계를 학습하는 데 사용되는 기술입니다. 입력 시퀀스의 모든 단어를 서로 다른 관련성 가중치로 가중 평균하여 표현하는 방법입니다.\n",
        "\n",
        "[계산 방법]\n",
        "\n",
        "1) 입력 시퀀스를 Query(Q), Key(K), Value(V)로 세 가지 선형 변환을 거칩니다. 이를 통해 각각의 단어들을 차원을 다르게하여 쿼리, 키, 밸류로 표현합니다.\n",
        "\n",
        "2) 쿼리(Q)와 키(K) 간의 유사도를 계산합니다. 일반적으로는 내적(dot-product)을 사용하여 유사도를 계산합니다.\n",
        "\n",
        "3) 유사도를 키(K)의 차원 수로 나누어, 스케일링(scaling)을 적용합니다. 스케일링은 유사도를 안정적으로 유지하기 위해 사용됩니다.\n",
        "\n",
        "4) 계산된 유사도를 소프트맥스(softmax) 함수를 통해 정규화합니다. 이로써 입력 시퀀스 내의 모든 단어들 간의 관련성 가중치를 얻을 수 있습니다.\n",
        "\n",
        "5) 정규화된 가중치와 키(K)에 대응하는 밸류(V)를 가중 평균하여 Self-Attention 값을 얻습니다. 이는 입력 시퀀스 내의 각 단어에 대해 중요도를 반영한 표현을 얻는 것을 의미합니다.\n",
        "\n",
        "Scaled Dot-Product Attention은 행렬 연산을 통해 병렬적으로 처리되기 때문에 다수의 단어들 간의 관계를 빠르게 계산할 수 있습니다. 이로 인해 Transformer 모델은 긴 시퀀스에 대해서도 비교적 높은 효율성을 유지할 수 있게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8X3P0lp02XQ"
      },
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the scaled dot-product attention mechanism.\n",
        "\n",
        "    This class provides a scaled dot-product attention mechanism as used in transformer models. It scales the dot products by the square root of the dimension of the attention keys, which helps in stabilizing gradients during backpropagation. Optionally, a mask can be applied to avoid attention to particular positions.\n",
        "\n",
        "    Attributes:\n",
        "    embedding_dim (int): The dimensionality of input embeddings.\n",
        "    attention_dim (int): The dimensionality of the attention space, typically the same as `embedding_dim`.\n",
        "    scale (torch.Tensor): The scaling factor for the attention scores, computed as the square root of `attention_dim`.\n",
        "\n",
        "    Parameters:\n",
        "    embedding_dim (int): Dimensionality of the input embeddings.\n",
        "    attention_dim (int): Size of the attention keys and values.\n",
        "\n",
        "    Methods:\n",
        "    forward(query, key, value, mask=None): Computes the attention values based on the provided query, key, and value tensors. Optional masking can be applied.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, attention_dim):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.scale = torch.sqrt(torch.tensor(attention_dim).float())\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Computes the attention values using scaled dot-product attention mechanism.\n",
        "\n",
        "        Parameters:\n",
        "        query (torch.Tensor): Query tensor of shape (batch_size, num_heads, sequence_length, attention_dim).\n",
        "        key (torch.Tensor): Key tensor of shape (batch_size, num_heads, sequence_length, attention_dim).\n",
        "        value (torch.Tensor): Value tensor of shape (batch_size, num_heads, sequence_length, attention_dim).\n",
        "        mask (torch.Tensor, optional): The mask's elements are 0 where the attention should be masked, and 1 otherwise.\n",
        "\n",
        "        Returns:\n",
        "        attention_value (torch.Tensor): The resulting tensor after applying attention, of shape (batch_size, num_heads, sequence_length, attention_dim).\n",
        "\n",
        "        Notes:\n",
        "        - The mask is applied to the attention scores before softmax, setting masked positions to a large negative value to minimize their effect in the softmax step.\n",
        "        \"\"\"\n",
        "        key = key.transpose(-2, -1)\n",
        "        attention_score = torch.matmul(query, key) / self.scale # (batch_size, num_heads, max_length, max_length)\n",
        "        attention_score = attention_score.masked_fill(mask == 0, -1e10)\n",
        "        attention_distribution = torch.softmax(attention_score, dim=-1)\n",
        "        attention_value = torch.matmul(attention_distribution, value) # (batch_size, num_heads, max_length, attention_dim)\n",
        "\n",
        "        return attention_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_JYdtf1JUW6"
      },
      "source": [
        "### 2-2-2. MultiHeadAttention\n",
        "\n",
        "\n",
        "![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/multi_head_attention.png)\n",
        "\n",
        "마찬가지로 Self-Attention 메커니즘 중 하나로, 입력 시퀀스의 다양한 관점을 캡처하기 위해 여러 개의 Attention 헤드를 병렬로 사용하는 기법입니다. Self-Attention 레이어를 여러 개의 헤드로 나누고, 각 헤드에서 병렬로 Self-Attention을 수행하여 다양한 정보를 효과적으로 추출합니다.\n",
        "\n",
        "[계산 방법]\n",
        "\n",
        "1) 입력 시퀀스를 여러 개의 서로 다른 헤드로 분리합니다. 각 헤드는 별도의 Query(Q), Key(K), Value(V) 선형 변환을 적용합니다. 이를 통해 서로 다른 특성을 가진 Query, Key, Value를 추출할 수 있습니다.\n",
        "\n",
        "2) 각 헤드에서는 Scaled Dot-Product Attention을 사용하여 서로 다른 관점으로 입력 시퀀스의 단어들 간의 관계를 학습합니다. 각 헤드는 서로 다른 관점의 정보를 캡처하고, 다양한 종류의 패턴을 인식할 수 있게 됩니다.\n",
        "\n",
        "3) 계산된 Self-Attention 결과를 다시 하나의 행렬로 결합합니다. 이를 통해 서로 다른 헤드의 정보를 종합하여 최종 Self-Attention 결과를 얻을 수 있습니다.\n",
        "\n",
        "4) 병렬로 동작하는 여러 헤드를 가짐으로써, 모델은 다양한 관점에서 입력 시퀀스를 살펴볼 수 있고, 각 단어에 대해 다양한 특징을 추출하여 보다 풍부한 표현을 얻을 수 있게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miuM4uVb05oZ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the Multi-Head Attention mechanism.\n",
        "\n",
        "    This class combines multiple attention heads to allow the model to jointly attend to information from different representation subspaces at different positions.\n",
        "    The input is transformed into multiple heads, each with its own set of linear transformations for queries, keys, and values, followed by the scaled dot-product attention.\n",
        "    The outputs of these heads are then concatenated and linearly transformed into the final output.\n",
        "\n",
        "    Attributes:\n",
        "    num_heads (int): The number of attention heads.\n",
        "    embedding_dim (int): The total dimension of the input embeddings.\n",
        "    attention_dim (int): Dimension of each attention head.\n",
        "\n",
        "    scaled_dot_product_attention (ScaledDotProductAttention): The attention mechanism used in each head.\n",
        "    Wq (nn.Linear): Linear transformation for query vectors.\n",
        "    Wk (nn.Linear): Linear transformation for key vectors.\n",
        "    Wv (nn.Linear): Linear transformation for value vectors.\n",
        "    Wo (nn.Linear): Linear transformation that combines outputs from all attention heads.\n",
        "\n",
        "    Parameters:\n",
        "    embedding_dim (int): Total dimensionality of the input embeddings.\n",
        "    num_heads (int): Number of heads to split the embedding_dim into.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, num_heads):\n",
        "        super(MultiHeadAttention, self,).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.attention_dim = self.embedding_dim // self.num_heads\n",
        "\n",
        "        self.scaled_dot_product_attention = ScaledDotProductAttention(embedding_dim, self.attention_dim)\n",
        "        self.Wq = nn.Linear(embedding_dim, self.attention_dim * num_heads)\n",
        "        self.Wk = nn.Linear(embedding_dim, self.attention_dim * num_heads)\n",
        "        self.Wv = nn.Linear(embedding_dim, self.attention_dim * num_heads)\n",
        "        self.Wo = nn.Linear(self.attention_dim * num_heads, embedding_dim)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "        Computes the output of the Multi-Head Attention layer.\n",
        "\n",
        "        Parameters:\n",
        "        query (torch.Tensor): Query tensor of shape (batch_size, sequence_length, embedding_dim).\n",
        "        key (torch.Tensor): Key tensor of shape (batch_size, sequence_length, embedding_dim).\n",
        "        value (torch.Tensor): Value tensor of shape (batch_size, sequence_length, embedding_dim).\n",
        "        mask (torch.Tensor, optional): Mask tensor (batch_size, 1, sequence_length, sequence_length) to exclude certain positions from attention.\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: The resulting tensor after applying multi-head attention, of shape (batch_size, sequence_length, embedding_dim).\n",
        "        \"\"\"\n",
        "        batch_size = query.size(0)\n",
        "        sequence_length = query.size(1)\n",
        "\n",
        "        query = self.Wq(query).reshape(batch_size, sequence_length, self.num_heads, self.attention_dim).transpose(1, 2)\n",
        "        key = self.Wk(key).reshape(batch_size, sequence_length, self.num_heads, self.attention_dim).transpose(1, 2)\n",
        "        value = self.Wv(value).reshape(batch_size, sequence_length, self.num_heads, self.attention_dim).transpose(1, 2)\n",
        "\n",
        "        attention_values = self.scaled_dot_product_attention(query, key, value, mask)   # (batch_size, num_heads, max_length, attention_dim)\n",
        "        attention_values = attention_values.transpose(1, 2).reshape(batch_size, sequence_length, self.num_heads * self.attention_dim)\n",
        "        output = self.Wo(attention_values)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XpnzO0cMkVQ"
      },
      "source": [
        "## 2-3. FeedForwardNeuralNetwork\n",
        "\n",
        "가장 간단한 형태의 FeedForward Network의 형태로 설명을 읽고 순서에 맞추어 구현해주시면 되겠습니다.\n",
        "![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/feed-forward.png)\n",
        "\n",
        "Position-wise Feed-Forward Network은 각 위치(position)별로 독립적으로 적용되는 두 개의 선형 변환과 활성화 함수로 구성되는 네트워크입니다. 모델의 비선형성을 증가시키고, 입력 시퀀스의 각 위치에서 다양한 특징을 추출하는 데 도움이 됩니다.\n",
        "\n",
        "[설명]\n",
        "\n",
        "1) 입력 시퀀스의 각 위치별로, 먼저 하나의 선형 변환을 적용합니다. 이는 입력 시퀀스의 각 위치별로 입력 차원을 다른 차원으로 매핑하는 역할을 합니다.\n",
        "\n",
        "2) 활성화 함수로 주로 ReLU(Rectified Linear Unit)가 사용됩니다. 이 활성화 함수는 비선형성을 도입하여 모델이 더 복잡한 관계를 학습할 수 있도록 도와줍니다.\n",
        "\n",
        "3) 두 번째 선형 변환을 적용합니다. 이는 ReLU 활성화 함수를 통과한 결과를 다시 다른 차원으로 매핑하여 최종적인 출력 차원을 얻는 역할을 합니다.\n",
        "\n",
        "- PoswiseFeedForwardNet은 다음과 같은 수식으로 표현될 수 있습니다:\n",
        "\n",
        "- $PoswiseFeedForwardNet(x)=ReLU(xW_1​ +b_1​)W_2​ +b_2\n",
        "​ $\n",
        "\n",
        "여기서\n",
        "$x$는 입력 시퀀스의 각 위치에 대한 벡터를 나타내고, $W_1$과 $b_1$은 첫 번째 선형 변환의 가중치 행렬과 편향 벡터, $W_2$와 $b_2$는 두 번째 선형 변환의 가중치 행렬과 편향 벡터를 나타냅니다.\n",
        "\n",
        "모델이 입력 시퀀스의 각 위치에서 다양한 특징을 추출하고, 비선형성을 도입하여 더 풍부한 표현을 학습할 수 있도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HXckFeA09p7"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNeuralNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a simple feed-forward neural network with one hidden layer as part of a transformer model.\n",
        "\n",
        "    This class defines a two-layer feed-forward neural network often used as the position-wise feed-forward network in transformer models. It includes a ReLU activation between the two linear layers, which allows the network to introduce non-linearity into the model's architecture, enhancing its learning capabilities.\n",
        "\n",
        "    Attributes:\n",
        "    W1 (nn.Linear): First linear transformation layer mapping from `embedding_dim` to `feed_forward_dim`.\n",
        "    W2 (nn.Linear): Second linear transformation layer mapping from `feed_forward_dim` back to `embedding_dim`.\n",
        "    relu (nn.ReLU): The ReLU activation function applied after the first linear transformation.\n",
        "\n",
        "    Parameters:\n",
        "    embedding_dim (int): Dimensionality of input and output embeddings.\n",
        "    feed_forward_dim (int): Dimensionality of the hidden layer.\n",
        "\n",
        "    Methods:\n",
        "    forward(inputs): Processes the input through the two linear layers and ReLU activation, producing the transformed output.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, feed_forward_dim, drop_prob):\n",
        "        super(FeedForwardNeuralNetwork, self).__init__()\n",
        "        self.W1 = nn.Linear(embedding_dim, feed_forward_dim)\n",
        "        self.W2 = nn.Linear(feed_forward_dim, embedding_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Computes the output of the feed-forward neural network.\n",
        "\n",
        "        Parameters:\n",
        "        inputs (torch.Tensor): The input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "        torch.Tensor: The output tensor after processing through the feed-forward neural network, matching the shape of the input tensor (batch_size, sequence_length, embedding_dim).\n",
        "        \"\"\"\n",
        "        outputs = self.W1(inputs)\n",
        "        outputs = self.relu(outputs)\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs = self.W2(outputs)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8A1nAvY1IXU"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(num_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmslkVSn1bow"
      },
      "outputs": [],
      "source": [
        "def create_padding_mask_for_attention(batch: Tensor, pad_idx: int) -> Tensor:\n",
        "    \"\"\"\n",
        "    Creates a square binary padding mask for the attention mechanism, where the mask indicates\n",
        "    whether each token in the input batch is a padding token or not. This mask can be used\n",
        "    in attention mechanisms to prevent attention to padding tokens.\n",
        "\n",
        "    This function assumes that the input tensor `batch` is a 2D tensor representing sequences of\n",
        "    tokens, where each element in the tensor is a token ID. The mask is constructed such that\n",
        "    it has dimensions suitable for multi-head attention, adding two additional dimensions\n",
        "    to cater to the heads and to square the mask.\n",
        "\n",
        "    Parameters:\n",
        "    - batch (Tensor): A 2D tensor of shape (batch_size, sequence_length), where each entry is a token ID.\n",
        "    - pad_idx (int): The token ID used to identify padding tokens in the batch.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: A 4D binary mask tensor of shape (batch_size, 1, sequence_length, sequence_length).\n",
        "              This tensor is placed on the same device as the input tensor `batch`.\n",
        "              A value of `True` at a position indicates that the corresponding token\n",
        "              is not a padding token and should be attended to, while `False` indicates\n",
        "              it is a padding token and should not be attended to in the attention layers.\n",
        "\n",
        "    \"\"\"\n",
        "    padding_mask = (batch != pad_idx)\n",
        "    sequence_length = batch.size(1)\n",
        "    padding_mask = padding_mask.unsqueeze(1).repeat(1, sequence_length, 1)\n",
        "    padding_mask = padding_mask.unsqueeze(1)\n",
        "\n",
        "    return padding_mask.to(device) # (batch_size, 1, max_length, max_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyAQ8D-d_FVi"
      },
      "source": [
        "### 2-4. Encoder\n",
        "**Encoder Layer를 직접 구현해봅니다!**\n",
        "\n",
        "앞 서 구현한 클래스를 바탕으로 Encoder Layer의 순서에 맞게 Encoder Layer를 구현해주시면 됩니다. 아래 이미지와 강의자료를 참고해주세요!\n",
        "\n",
        "Encoder Layer는 입력 시퀀스의 단어들을 인코딩하여 중간 표현을 생성하는 역할을 합니다.\n",
        "\n",
        "1) 입력 단어들에 대해 주변 단어들과의 관련성을 고려한 Self-Attention을 수행하여 풍부한 문맥 정보를 추출합니다.\n",
        "\n",
        "2) Feed-Forward Network를 통해 비선형성을 도입하고 다양한 특징을 추출합니다.\n",
        "\n",
        "3) Layer 간 잔차 연결과 Layer Normalization으로 Gradient Vanishing 문제를 완화하고, 모델 학습을 돕습니다.\n",
        "\n",
        "![](https://raw.githubusercontent.com/paul-hyun/paul-hyun.github.io/master/assets/2019-12-19/encoder.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZCLBbOf2IvD"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Represents a single layer of a Transformer encoder as described in the paper \"Attention is All You Need\".\n",
        "    This layer includes two sub-layers: a multi-head self-attention mechanism, and a position-wise fully connected feed-forward network.\n",
        "    Each of these sub-layers has a residual connection around it followed by layer normalization.\n",
        "\n",
        "    Attributes:\n",
        "        multi_head_attention (MultiHeadAttention): The multi-head attention component which performs self-attention over the input batch.\n",
        "        feed_forward_neural_network (FeedForwardNeuralNetwork): The feed-forward neural network that transforms the output of the attention mechanism.\n",
        "        norm1 (LayerNormalization): The first layer normalization that is applied after the attention mechanism and before the addition of the residual connection.\n",
        "        norm2 (LayerNormalization): The second layer normalization that is applied after the feed-forward network and before the addition of the residual connection.\n",
        "\n",
        "    Parameters:\n",
        "        embedding_dim (int): The dimensionality of the input embeddings.\n",
        "        feed_forward_dim (int): The dimensionality of the hidden layer in the feed-forward neural network.\n",
        "        num_heads (int): The number of heads in the multi-head attention mechanism.\n",
        "\n",
        "    Methods:\n",
        "        forward(batch: Tensor, padding_mask: Tensor) -> Tensor:\n",
        "            Processes the input `batch` through one encoder layer including multi-head attention,\n",
        "            layer normalization, and a feed-forward network to produce an output tensor.\n",
        "\n",
        "            Parameters:\n",
        "                batch (Tensor): The input tensor to the encoder layer with shape\n",
        "                    (batch_size, sequence_length, embedding_dim).\n",
        "                padding_mask (Tensor): The padding mask tensor with shape\n",
        "                    (batch_size, 1, sequence_length, sequence_length), used in the multi-head attention to avoid\n",
        "                    attending to padding positions.\n",
        "\n",
        "            Returns:\n",
        "                Tensor: The output of the encoder layer, which has the same shape as the input\n",
        "                    batch (batch_size, sequence_length, embedding_dim).\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, feed_forward_dim, num_heads, drop_prob):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.multi_head_attention = MultiHeadAttention(embedding_dim, num_heads)\n",
        "        self.norm1 = LayerNormalization(embedding_dim)\n",
        "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "        self.feed_forward_neural_network = FeedForwardNeuralNetwork(embedding_dim, feed_forward_dim, drop_prob=drop_prob)\n",
        "        self.norm2 = LayerNormalization(embedding_dim)\n",
        "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
        "\n",
        "    def forward(self, x, padding_mask):\n",
        "        _x = x\n",
        "        x = self.multi_head_attention(x, x, x, padding_mask)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.norm1(x + _x)\n",
        "\n",
        "        _x = x\n",
        "        x = self.feed_forward_neural_network(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.norm2(x + _x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuDYkfeL_FVj"
      },
      "source": [
        "\n",
        "**Encoder Layer를 구현했다면 전체 Transformer Encoder의 구조를 구현해봅시다.**\n",
        "\n",
        "해당 Encoder가 전체 input부터 output까지 출력하는 전체 구조입니다. 역시나 앞 서 구현한 클래스들을 바탕으로 구현해주시면 되고 Transformer Encoder에는 Encoder Layer(Block)이 여러 개(논문 기준 6개)가 포함된다고 설명드렸습니다. 이 점에 유의해서 구현해주세요!\n",
        "\n",
        "1) 입력에 대한 Position 값을 구하기: 각 단어의 상대적인 위치를 나타내는 Positional Encoding을 계산합니다.\n",
        "\n",
        "2) Input Embedding과 Position Embedding 더하기: 입력 시퀀스의 단어들을 임베딩하여 벡터로 표현한 후, Positional Encoding을 더합니다. 이를 통해 입력 시퀀스의 단어들은 고유한 위치 정보를 가진 임베딩으로 변환됩니다.\n",
        "\n",
        "3) 입력에 대한 Attention Pad Mask 구하기: Self-Attention 레이어에서 패딩 부분에 대한 마스크를 생성합니다. 이렇게 함으로써 모델이 패딩 부분을 무시하고, 실제 입력에만 집중할 수 있도록 돕습니다.\n",
        "\n",
        "4) for 루프를 돌며 각 layer를 실행하기: 여러 개의 EncoderLayer로 구성된 스택을 순차적으로 거칩니다. 각 EncoderLayer는 입력 시퀀스에 대한 인코딩을 수행하고, 다음 EncoderLayer로 전달하기 위해 중간 결과를 출력합니다.\n",
        "\n",
        "5) layer의 입력은 이전 layer의 출력 값: 스택의 첫 번째 EncoderLayer를 거칠 때는 이전 layer가 없으므로, Input Embedding과 Position Embedding의 결과가 첫 번째 EncoderLayer의 입력으로 사용됩니다. 이후의 EncoderLayer들은 이전 layer의 출력 값을 입력으로 받아 처리합니다.\n",
        "\n",
        "이렇게 입력 시퀀스에 대한 인코딩을 위해 Positional Encoding, Self-Attention, Residual Connection 등의 기법을 사용하여 입력 정보를 풍부하게 표현합니다. 이 과정을 여러 번 쌓아 올려서 Encoder를 형성하며, 최종적으로 입력 시퀀스의 문맥 정보를 잘 반영한 중간 표현을 얻습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvfIQzx42JRS"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A Transformer encoder module for processing sequences using multiple encoder layers.\n",
        "\n",
        "    This class is a PyTorch module that utilizes a stack of transformer encoder layers to process\n",
        "    input sequences. It first applies positional embeddings using a TransformerEmbedding layer\n",
        "    before passing the data through multiple transformer encoder layers.\n",
        "\n",
        "    Attributes:\n",
        "        transformer_embedding (TransformerEmbedding): An embedding layer that adds position\n",
        "            embeddings to the token embeddings.\n",
        "        encoder_layers (nn.ModuleList): A list of encoder layers (instances of EncoderLayer)\n",
        "            used to process the input data sequentially.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): The size of the vocabulary (number of unique tokens).\n",
        "        sequence_length (int): The sequence length of input sequences.\n",
        "        embedding_dim (int): The dimensionality of token embeddings.\n",
        "        feed_forward_dim (int): The dimensionality of the feed-forward networks in the encoder layers.\n",
        "        num_heads (int): The number of attention heads in each encoder layer.\n",
        "        num_layers (int): The number of encoder layers in the module.\n",
        "\n",
        "    Methods:\n",
        "        forward(x, padding_mask):\n",
        "            Processes input data through the transformer embedding and each encoder layer sequentially.\n",
        "\n",
        "            Args:\n",
        "                x (Tensor): The input tensor containing token indices, shape [batch_size, sequence_length].\n",
        "                padding_mask (Tensor): The padding mask for the input tensor, shape [batch_size, sequence_length].\n",
        "\n",
        "            Returns:\n",
        "                Tensor: The output tensor from the final encoder layer, shape [batch_size, sequence_length, embedding_dim].\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, sequence_length, embedding_dim, feed_forward_dim, num_heads, num_layers, drop_prob):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.transformer_embedding = TransformerEmbedding(vocab_size, embedding_dim, sequence_length)\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(embedding_dim, feed_forward_dim, num_heads, drop_prob)\n",
        "                                             for _ in range(num_layers)])\n",
        "\n",
        "    def forward(self, x, padding_mask):\n",
        "        x = self.transformer_embedding(x)\n",
        "\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x, padding_mask)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **여기서부터 구현 시작하시면 됩니다!!**"
      ],
      "metadata": {
        "id": "7n0Lz6zLJsfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-5. Decoder\n",
        "\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/decoder.PNG)\n"
      ],
      "metadata": {
        "id": "CiL_saWWl-zo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-5-1. Look-Ahead Mask\n",
        "\n",
        "위 그림과 같이 decoder도 encoder와 동일하게 embedding layer와 positional encoding 을 거친 후의 문장 행렬이 입력됩니다. Transformer 또한 seq2seq와 마찬가지로 교사 강요(Teacher Forcing)을 사용하여 훈련되므로 학습 과정에서 decoder는 번역할 문장에 해당되는 `<sos> je suis étudiant`의 문장 행렬을 한 번에 입력받습니다. 그리고 decoder는 이 문장 행렬로부터 각 시점의 단어를 예측하도록 훈련됩니다.\n",
        "\n",
        "여기서 문제가 있습니다. seq2seq의 decoder에 사용되는 RNN 계열의 신경망은 입력 단어를 매 시점마다 순차적으로 입력받으므로 다음 단어 예측에 현재 시점을 포함한 이전 시점에 입력된 단어들만 참고할 수 있습니다. 반면, Transformer는 문장 행렬로 입력을 한 번에 받으므로 현재 시점의 단어를 예측하고자 할 때, 입력 문장 행렬로부터 미래 시점의 단어까지도 참고할 수 있는 현상이 발생합니다. 가령, suis를 예측해야 하는 시점이라고 해봅시다. RNN 계열의 seq2seq의 decoder라면 현재까지 decoder에 입력된 단어는 <sos>와 je뿐일 것입니다. 반면, Transformer는 이미 문장 행렬로 `<sos> je suis étudiant`를 입력받았습니다.\n",
        "\n",
        "이를 위해 Transformer의 decoder에서는 현재 시점의 예측에서 현재 시점보다 미래에 있는 단어들을 참고하지 못하도록 룩-어헤드 마스크(look-ahead mask)를 도입했습니다. 직역하면 '미리보기에 대한 마스크'입니다.\n",
        "\n",
        "룩-어헤드 마스크(look-ahead mask)는 decoder의 첫번째 sublayer에서 이루어집니다. Decoder의 첫번째 sublayer인 multi-head self-attention layer는 encoder의 첫번째 sublayer인 multi-head self-attention layer와 동일한 연산을 수행합니다. 오직 다른 점은 attention score 행렬에서 masking을 적용한다는 점만 다릅니다. 우선 다음과 같이 self-attention을 통해 attention score 행렬을 얻습니다.\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/decoder_attention_score_matrix.PNG)\n",
        "\n",
        "\n",
        "이제 자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 다음과 같이 masking합니다.\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/%EB%A3%A9%EC%96%B4%ED%97%A4%EB%93%9C%EB%A7%88%EC%8A%A4%ED%81%AC.PNG)\n",
        "\n",
        "Masking 된 후의 attention score 행렬의 각 행을 보면 자기 자신과 그 이전 단어들만을 참고할 수 있음을 볼 수 있습니다. 그 외에는 근본적으로 self-attention이라는 점과, multi-head attention을 수행한다는 점에서 encoder의 첫번째 sublayer와 같습니다.\n",
        "\n",
        "Look-ahead mask의 구현에 대해 알아봅시다. Look-ahead mask는 padding mask와 마찬가지로 앞서 구현한 scaled dot product attention 함수에 mask라는 인자로 전달됩니다. Padding mask를 써야하는 경우에는 scaled dot product attention 함수에 padding mask를 전달하고, look-ahead mask를 써야하는 경우에는 scaled dot product attention 함수에 look-ahead mask를 전달합니다.\n",
        "\n",
        "이때 look-ahead mask를 한다고해서 padding mask가 불필요한 것이 아니므로 look-ahead mask는 padding mask를 포함하도록 구현합니다. Look-ahead mask를 구현하는 방법은 padding mask 때와 마찬가지로 masking을 하고자 하는 위치에는 0을, masking을 하지 않는 위치에는 1을 리턴하도록 합니다."
      ],
      "metadata": {
        "id": "3MU-tAjRKMq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask_for_attention(tgt_batch: torch.Tensor, pad_idx: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Creates a look-ahead mask to prevent positions from attending to future positions in the\n",
        "    target sequence during self-attention computations in the Transformer decoder. This mask is combined\n",
        "    with a padding mask to ensure that padding positions are also ignored.\n",
        "\n",
        "    Parameters:\n",
        "    - tgt_batch (Tensor): A tensor of shape (batch_size, tgt_length) containing target\n",
        "      sequences where each element is a token index.\n",
        "    - pad_idx (int): The index used for padding tokens within the batch.\n",
        "\n",
        "    Returns:\n",
        "    - Tensor: A combined look-ahead and padding mask tensor of shape\n",
        "      (batch_size, 1, tgt_length, tgt_length). This tensor contains Boolean values, where `True`\n",
        "      indicates that attention is allowed and `False` indicates it is blocked.\n",
        "\n",
        "    Example:\n",
        "    ```python\n",
        "    import torch\n",
        "\n",
        "    # Assuming the presence of 'create_look_ahead_mask_for_attention' in the current script or imported module\n",
        "    # and 'pad_idx' is known (e.g., if pad_idx is 0)\n",
        "\n",
        "    # Example tensor simulating a batch of target sequences with batch_size=2 and tgt_length=3\n",
        "    # Assume padding index (pad_idx) is 0, and the actual data does not include this index in this example\n",
        "    tgt_batch = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "    # Generate the look-ahead mask\n",
        "    mask = create_look_ahead_mask_for_attention(tgt_batch, pad_idx=0)\n",
        "\n",
        "    # Print the mask output, showing blocked future positions and allowing current and past positions\n",
        "    print(mask)\n",
        "    # Output:\n",
        "    # tensor([[[[ True, False, False],\n",
        "    #           [ True,  True, False],\n",
        "    #           [ True,  True,  True]]],\n",
        "    #\n",
        "    #         [[[ True, False, False],\n",
        "    #           [ True,  True, False],\n",
        "    #           [ True,  True,  True]]]], dtype=torch.bool)\n",
        "    ```\n",
        "\n",
        "    Notes:\n",
        "    - Ensure that `tgt_batch` is on the same device as the `device` variable defined in the function's scope or\n",
        "      configure the function to automatically detect the device from `tgt_batch`.\n",
        "    - This function handles both the triangular look-ahead masking and the padding by combining them into a single mask.\n",
        "    \"\"\"\n",
        "\n",
        "    batch_size, tgt_length = tgt_batch.size()\n",
        "\n",
        "    # TODO: look-ahead mask 구현\n",
        "\n",
        "\n",
        "    tgt_padding_mask = create_padding_mask_for_attention(tgt_batch, pad_idx)\n",
        "    tgt_attention_mask = tgt_attention_mask.to(torch.int) & tgt_padding_mask.to(torch.int)\n",
        "\n",
        "    return tgt_attention_mask"
      ],
      "metadata": {
        "id": "_A9M2suum0Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-5-2. Decoder Layer\n",
        "\n",
        "**Decoder Layer를 직접 구현해봅니다!**\n",
        "\n",
        "이제 Encoder Layer를 통해 변환된 정보를 사용하여 Decoder Layer를 구현해보겠습니다. 아래 내용과 강의자료를 참고하시어 Decoder를 구현하시면 됩니다.\n",
        "\n",
        "Decoder는 encoder에서 변환된 정보를 바탕으로 sequence를 생성하는 과정에서 중요한 역할을 합니다.\n",
        "\n",
        "1. 입력된 sequence와 Encoder에서 전달된 출력을 이용해 Masked Self-Attention을 수행합니다. 이 과정에서 미래 시점의 정보가 예측에 영향을 주지 않도록 masking 처리를 하여 현재 및 이전 단어들만 고려됩니다.\n",
        "\n",
        "2. Encoder-Decoder Attention에서는 Decoder의 출력을 query로 하고, Encoder의 출력을 key와 value로 사용합니다. 이는 Decoder가 Encoder로부터 전달된 output을 통해 필요한 정보를 선택적으로 추출할 수 있게 합니다.\n",
        "\n",
        "3. Masked Self-Attention과 Encoder-Decoder Attention 후에는 추가적인 Feed-Forward Network를 통해 출력 sequence의 각 단어에 대한 최종 예측을 생성하기 전에 nonlinear transformation을 적용합니다.\n",
        "\n",
        "4. 각 단계마다 residual connection과 layer normalization을 사용하여 모델의 학습 안정성을 증진시키고, Gradient Vanishing 문제를 방지합니다.\n",
        "\n",
        "Decoder Layer는 이러한 단계들을 통해 입력 sequence에 따른 결과 sequence를 적절히 생성하며, 최종적으로 원하는 타겟 sequence를 생성하는 데 중요한 역할을 합니다.\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/decoder.PNG)"
      ],
      "metadata": {
        "id": "oJ_r4mHBOm57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a single layer of a Transformer decoder as described in \"Attention is All You Need\" paper.\n",
        "\n",
        "    This layer includes three main components: masked multi-head self-attention, encoder-decoder attention,\n",
        "    and a position-wise feed-forward neural network. Each of these components is followed by layer normalization\n",
        "    and dropout for regularization.\n",
        "\n",
        "    Attributes:\n",
        "        masked_multi_head_attention (MultiHeadAttention): The multi-head attention mechanism with masking,\n",
        "            used for self-attention within the decoder.\n",
        "        norm1 (LayerNormalization): Normalization layer following the masked multi-head attention.\n",
        "        dropout1 (nn.Dropout): Dropout layer after the first normalization layer.\n",
        "\n",
        "        enc_dec_attention (MultiHeadAttention): Multi-head attention mechanism used between the encoder\n",
        "            and decoder to focus on relevant parts of the input sequence.\n",
        "        norm2 (LayerNormalization): Normalization layer following the encoder-decoder attention.\n",
        "        dropout2 (nn.Dropout): Dropout layer after the second normalization layer.\n",
        "\n",
        "        feed_forward_neural_network (FeedForwardNeuralNetwork): Position-wise feed-forward neural network,\n",
        "            applies a fully connected layer to each position separately and identically.\n",
        "        norm3 (LayerNormalization): Normalization layer following the feed-forward neural network.\n",
        "        dropout3 (nn.Dropout): Dropout layer after the third normalization layer.\n",
        "\n",
        "    Parameters:\n",
        "        embedding_dim (int): Dimensionality of the input token embeddings.\n",
        "        feed_forward_dim (int): Dimensionality of the inner layer of the feed-forward neural network.\n",
        "        num_heads (int): Number of attention heads.\n",
        "        drop_prob (float): Dropout probability.\n",
        "\n",
        "    Methods:\n",
        "        forward(dec, enc, src_mask, trg_mask): Defines the computation performed at every call. It processes\n",
        "            the input through the decoder layer transformations and returns the transformed output.\n",
        "\n",
        "        Parameters for forward:\n",
        "            dec (Tensor): The sequence of embeddings representing the decoder's input.\n",
        "            enc (Tensor): The sequence of embeddings from the encoder's output.\n",
        "            src_mask (Tensor): The mask applied to the encoder's output.\n",
        "            trg_mask (Tensor): The mask applied to the decoder's input to prevent attending to future tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: DecoderLayer 구현\n",
        "    # 화이팅!\n",
        "\n",
        "    def __init__(self, embedding_dim, feed_forward_dim, num_heads, drop_prob):\n",
        "\n",
        "\n",
        "    def forward(self, dec, enc, src_mask, trg_mask):\n",
        "\n",
        "\n",
        "        return\n",
        "'''"
      ],
      "metadata": {
        "id": "m8EatyP8mlaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decoder Layer를 구현했다면 전체 Transformer Decoder의 구조를 구현해봅시다.**\n",
        "\n",
        "해당 Decoder는 Encoder로부터의 출력과 함께 타겟 sequecne를 입력받아 최종 출력 sequence를 생성합니다. Decoder 역시 여러 개의 Decoder Layer(Block)로 구성되어 있습니다 (논문 기준 6개). 각 단계를 철저히 이해하고 구현해 주세요.\n",
        "\n",
        "1.  타겟 입력에 대한 Position 값을 구하기: 타겟 입력 sequence의 각 단어에 대해 Positional Encoding을 계산합니다.\n",
        "\n",
        "2. Target Embedding과 Position Embedding 더하기: 타겟 sequence의 단어들을 embedding하여 벡터로 표현한 후, 해당 Positional Encoding을 더합니다. 이렇게 함으로써 각 단어는 위치 정보가 포함된 embedding으로 변환됩니다.\n",
        "\n",
        "3. 입력에 대한 Masked Attention Pad Mask 구하기: Decoder의 첫 번째 Self-Attention layer에서는 자신보다 뒤에 오는 단어들을 참조하지 않도록 masking 처리를 합니다. 이는 타겟 sequence의 예측 중 미래의 정보를 참조하는 것을 방지하기 위함입니다.\n",
        "\n",
        "4. Encoder-Decoder Attention을 통해 Encoder의 출력과 결합: 각 DecoderLayer는 Encoder의 출력을 사용하여 Encoder-Decoder Attention를 실행합니다. 이 과정에서 Decoder는 Encoder로부터 필요한 정보를 효율적으로 추출하여 타겟 sequence의 생성을 최적화합니다.\n",
        "\n",
        "5. layer의 입력은 이전 layer의 출력 값: 스택의 첫 번째 DecoderLayer를 거칠 때는 이전 layer가 없으므로, Target Embedding과 Position Embedding의 결과가 첫 번째 DecoderLayer의 입력으로 사용됩니다. 이후의 DecoderLayer들은 이전 layer의 출력 값을 입력으로 받아 처리합니다.\n",
        "\n",
        "6. for 루프를 돌며 각 layer를 실행하기: 여러 개의 DecoderLayer로 구성된 스택을 순차적으로 거칩니다. 각 DecoderLayer는 타겟 입력 sequence를 기반으로 연산을 수행하고, 다음 DecoderLayer로 중간 결과를 전달합니다.\n",
        "\n",
        "Decoder는 이러한 과정을 거쳐 최종적으로 각 타겟 sequence의 단어를 예측하는 데 필요한 정보를 통합하고, 전체 sequence를 생성하는 중요한 역할을 수행합니다."
      ],
      "metadata": {
        "id": "VXzsFYoUSkkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the decoder part of a Transformer model, which processes the encoded input\n",
        "    and generates an output sequence. The decoder is composed of multiple layers of a custom\n",
        "    `DecoderLayer`, a transformer embedding layer at the input, and a final dense layer that\n",
        "    outputs the probability distribution over a vocabulary.\n",
        "\n",
        "    Attributes:\n",
        "        transformer_embedding (TransformerEmbedding): Embedding layer for input tokens that\n",
        "            also adds positional encodings.\n",
        "        decoder_layers (nn.ModuleList): A list of `DecoderLayer` instances, each of which applies\n",
        "            masked multi-head attention, encoder-decoder attention, and a feed-forward network.\n",
        "        dense_layer (nn.Linear): A linear transformation applied to the outputs of the decoder layers,\n",
        "            mapping them to the vocabulary size.\n",
        "\n",
        "    Parameters:\n",
        "        vocab_size (int): The size of the vocabulary.\n",
        "        sequence_length (int): The maximum length of input sequences.\n",
        "        embedding_dim (int): The dimensionality of the embedding space.\n",
        "        feed_forward_dim (int): The dimensionality of the inner layer of the feed-forward neural network in the decoder.\n",
        "        num_heads (int): The number of heads in the multi-head attention mechanisms.\n",
        "        num_layers (int): The number of `DecoderLayer` modules in the decoder.\n",
        "        drop_prob (float): The dropout probability used in dropout layers within the decoder layers.\n",
        "\n",
        "    Methods:\n",
        "        forward(dec, enc, src_mask, trg_mask): Defines the computation performed at every call.\n",
        "            It processes the decoder input through the embedding layer, all the decoder layers in sequence,\n",
        "            and then through the dense layer to produce the final output logits.\n",
        "\n",
        "        Parameters for forward:\n",
        "            dec (Tensor): The sequence of input tokens (indices) for the decoder.\n",
        "            enc (Tensor): The sequence of embeddings output by the encoder, which acts as context for the decoder.\n",
        "            src_mask (Tensor): The mask to be applied to the encoder outputs to prevent attention to certain positions.\n",
        "            trg_mask (Tensor): The mask applied to the decoder inputs to prevent attention to subsequent positions\n",
        "                               in the sequence (used to enforce causality in the decoder).\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Decoder 구현\n",
        "    def __init__(self, vocab_size, sequence_length, embedding_dim, feed_forward_dim, num_heads, num_layers, drop_prob):\n",
        "\n",
        "\n",
        "    def forward(self, dec, enc, src_mask, trg_mask):\n",
        "\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "Ks2o4LzImnES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2-6. Transformer 완성\n",
        "\n",
        "\n",
        "![](https://wikidocs.net/images/page/31379/transformer_attention_overview.PNG)\n"
      ],
      "metadata": {
        "id": "fZGzUuqdsykd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Transformer model which consists of an encoder and a decoder.\n",
        "    The Transformer model is designed to handle sequences of tokens from two languages (source and target),\n",
        "    translating from the source language to the target language. This class is suitable for tasks such as\n",
        "    machine translation.\n",
        "\n",
        "    Attributes:\n",
        "        encoder (Encoder): The Encoder part of the Transformer model that processes the input in the source language.\n",
        "        decoder (Decoder): The Decoder part that generates output in the target language based on the encoder's output.\n",
        "\n",
        "    Parameters:\n",
        "        vocab_size (dict): A dictionary with keys as 'src_language' and 'tgt_language' representing\n",
        "                           the sizes of the vocabularies for the source and target languages, respectively.\n",
        "        sequence_length (int): The maximum length of the sequences to be processed.\n",
        "        embedding_dim (int): The dimensionality of the embeddings used for both the encoder and decoder.\n",
        "        feed_forward_dim (int): The dimensionality of the feed-forward network's inner layer in both the encoder\n",
        "                                and decoder.\n",
        "        num_heads (int): The number of heads in the multi-head attention mechanisms within the encoder and decoder.\n",
        "        num_layers (int): The number of layers in both the encoder and decoder.\n",
        "        drop_prob (float): The dropout probability used in dropout layers within both the encoder and decoder.\n",
        "\n",
        "    Methods:\n",
        "        forward(src_batch, tgt_batch, src_padding_mask, tgt_attention_mask): Defines the computation performed at\n",
        "            every call, processing the input through the encoder and decoder sequentially.\n",
        "\n",
        "        Parameters for forward:\n",
        "            src_batch (Tensor): Batch of input sequences for the encoder, typically representing token indices\n",
        "                                in the source language.\n",
        "            tgt_batch (Tensor): Batch of target sequences for the decoder, typically representing token indices\n",
        "                                in the target language to be predicted.\n",
        "            src_padding_mask (Tensor): A mask for the source sequences, used in the encoder to ignore padding tokens\n",
        "                                       during attention calculations.\n",
        "            tgt_attention_mask (Tensor): A mask for the target sequences used in the decoder to prevent attending\n",
        "                                         to future tokens (ensuring causality) and to ignore padding tokens.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, sequence_length, embedding_dim, feed_forward_dim, num_heads, num_layers, drop_prob):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder = Encoder(vocab_size[src_language], sequence_length, embedding_dim, feed_forward_dim, num_heads, num_layers, drop_prob)\n",
        "        self.decoder = Decoder(vocab_size[tgt_language], sequence_length, embedding_dim, feed_forward_dim, num_heads, num_layers, drop_prob)\n",
        "\n",
        "\n",
        "    def forward(self, src_batch, tgt_batch, src_padding_mask, tgt_attention_mask):\n",
        "        # TODO: Transformer forward 함수 구현\n",
        "        # 1~3 lines\n",
        "\n",
        "        return"
      ],
      "metadata": {
        "id": "-iEGmqNvmpBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Machine Translation with Multi30k"
      ],
      "metadata": {
        "id": "tZx5e9wJTmbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-1. Multi30k\n",
        "\n",
        "Transformer 모델 구현을 완료하였으니, Multi30k 데이터셋을 활용해 기계 번역을 해봅시다. 데이터셋 관련 변수는 다음과 같습니다.\n",
        "\n",
        "1. **`src_language`와 `tgt_language`**:\n",
        "   - `src_language = \"de\"`: 이 변수는 source language를 나타냅니다. 여기서 `\"de\"`는 독일어를 의미합니다. 모델은 독일어 문장을 입력으로 받아 처리합니다.\n",
        "   - `tgt_language = \"en\"`: 이 변수는 target language를 나타냅니다. 여기서 `\"en\"`은 영어를 의미합니다. 모델의 목표는 입력된 독일어 문장을 영어로 번역하는 것입니다.\n",
        "\n",
        "2. **인덱스 변수 (`unk_idx`, `sos_idx`, `eos_idx`, `pad_idx`)**:\n",
        "   - `unk_idx = 0`: \"unknown index\"의 약자로, 어휘에 없는 단어를 나타내는 특수 토큰의 인덱스입니다. 모델이 학습 중에 또는 번역 중에 알 수 없는 단어를 만났을 때 이 인덱스를 사용합니다.\n",
        "   - `sos_idx = 1`: \"start of sentence index\"의 약자로, 문장의 시작을 나타내는 특수 토큰의 인덱스입니다. 각 번역 문장을 시작할 때 이 인덱스를 사용하여 모델이 문장의 시작을 인식할 수 있게 합니다.\n",
        "   - `eos_idx = 2`: \"end of sentence index\"의 약자로, 문장의 종료를 나타내는 특수 토큰의 인덱스입니다. 모델이 문장의 끝에 도달했음을 나타내기 위해 사용됩니다.\n",
        "   - `pad_idx = 3`: \"padding index\"의 약자로, 입력 데이터의 길이를 맞추기 위해 사용하는 특수 토큰의 인덱스입니다. 배치 내의 모든 문장을 동일한 길이로 맞추기 위해 사용되며, 모델 학습에는 영향을 주지 않아야 합니다.\n",
        "\n",
        "이 변수들은 Transformer 모델이 기계 번역 작업을 수행할 때 중요한 역할을 합니다. 문장을 처리하고, 모델이 학습할 수 있도록 적절한 토큰을 제공하며, 데이터의 일관성을 유지하는 데 필수적인 요소입니다. 아래 코드는 수정하지 말고 그대로 사용해주세요."
      ],
      "metadata": {
        "id": "TvDrFxCKUhPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src_language = \"de\"\n",
        "tgt_language = \"en\"\n",
        "unk_idx, sos_idx, eos_idx, pad_idx = 0, 1, 2, 3"
      ],
      "metadata": {
        "id": "-ZPW_yctnAQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi30k 데이터셋의 구조를 확인해보세요:"
      ],
      "metadata": {
        "id": "rNwLiGSaWI6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "\n",
        "multi_train = Multi30k(split=('train'), language_pair=(src_language, tgt_language))\n",
        "\n",
        "for i, (de, en) in enumerate(multi_train):\n",
        "    if i == 5: break\n",
        "    print(f\"Index:{i}, Deutsche: {de}, English: {en}\")"
      ],
      "metadata": {
        "id": "rF-ISakNVx86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-2. DataLoader"
      ],
      "metadata": {
        "id": "yryauIRWW0ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataLoader 코드입니다. TODO 부분을 채워주세요."
      ],
      "metadata": {
        "id": "bIbrd3zUXErY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataLoader:\n",
        "    \"\"\"\n",
        "    Custom data loader designed for handling and preprocessing textual data for neural network training,\n",
        "    especially in sequence-to-sequence models such as machine translation between source and target languages.\n",
        "\n",
        "    Attributes:\n",
        "        src_language (str): Source language identifier.\n",
        "        tgt_language (str): Target language identifier.\n",
        "        max_length (int): Maximum length of token sequences to consider.\n",
        "        special_symbols (list): List of special symbols to be included in vocabulary (e.g., '<sos>', '<eos>').\n",
        "        tokenizer (dict): Dictionary of tokenization functions keyed by language.\n",
        "\n",
        "    Methods:\n",
        "        yield_tokens(tokenized_sentences: list) -> Iterator:\n",
        "            Yields tokenized sentences one by one, primarily used for vocabulary building.\n",
        "\n",
        "        longer_than_max_length(tokens: list) -> bool:\n",
        "            Checks if the token list exceeds the maximum allowed sequence length.\n",
        "\n",
        "        tokenize_seq_and_add_special_symbols(raw_text: str, language: str) -> list:\n",
        "            Tokenizes the input text for a specified language and appends special symbols at the start and end.\n",
        "\n",
        "        transform_tokens_to_tensor(tokens: list, language: str) -> Tensor:\n",
        "            Converts a list of tokens into a tensor of token indices according to the vocabulary.\n",
        "\n",
        "        build_vocab(text_data: Iterable[Tuple[str, str]]) -> tuple:\n",
        "            Builds vocabularies for the source and target languages based on provided paired text data.\n",
        "\n",
        "        make_dataloader(text_data: Iterable[Tuple[str, str]], batch_size: int) -> DataLoader:\n",
        "            Creates a DataLoader with batches of preprocessed, tokenized, and padded text data suitable\n",
        "            for model training. Handles tokenization, conversion to tensor, and padding within this method.\n",
        "\n",
        "    Parameters:\n",
        "        src_language (str): Identifier or name of the source language.\n",
        "        tgt_language (str): Identifier or name of the target language.\n",
        "        max_length (int): The maximum allowed length of token sequences.\n",
        "        special_symbols (list): Special symbols to be explicitly included in vocabularies.\n",
        "        tokenizer (dict): A dictionary containing tokenization functions keyed by their respective language tags.\n",
        "\n",
        "    Example usage:\n",
        "        # Assuming 'english_tokenizer' and 'spanish_tokenizer' are predefined tokenization functions:\n",
        "        loader = CustomDataLoader(\n",
        "            src_language='English', tgt_language='Spanish', max_length=100,\n",
        "            special_symbols=['<sos>', '<eos>', '<pad>', '<unk>'], tokenizer={'English': english_tokenizer, 'Spanish': spanish_tokenizer}\n",
        "        )\n",
        "    \"\"\"\n",
        "    def __init__(self, src_language: str, tgt_language: str, max_length: int, special_symbols: list, tokenizer: dict):\n",
        "        self.src_language = src_language\n",
        "        self.tgt_language = tgt_language\n",
        "        self.max_length = max_length\n",
        "        self.special_symbols = special_symbols\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "\n",
        "    def yield_tokens(self, tokenized_sentences: list):\n",
        "        for sentence in tokenized_sentences:\n",
        "            yield sentence\n",
        "\n",
        "    def longer_than_max_length(self, tokens: list) -> bool:\n",
        "        return len(tokens) > self.max_length\n",
        "\n",
        "    def tokenize_seq_and_add_special_symbols(self, raw_text: str, language: str) -> list:\n",
        "        tokens = self.tokenizer[language](raw_text)\n",
        "\n",
        "        # TODO: Add start and end token\n",
        "        tokens = ['<>'] + tokens + ['<>']\n",
        "        return tokens\n",
        "\n",
        "    def transform_tokens_to_tensor(self, tokens: list, language: str) -> Tensor:\n",
        "        indices = [vocab[language][token] for token in tokens]\n",
        "        tensor = torch.tensor(indices, dtype=torch.long)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "\n",
        "    def build_vocab(self, text_data) -> tuple:\n",
        "        sentences = {self.src_language: [], self.tgt_language: []}\n",
        "        vocab, vocab_size = {}, {}\n",
        "\n",
        "        for src_sentence, tgt_sentence in text_data:\n",
        "            src_sentence = self.tokenize_seq_and_add_special_symbols(src_sentence, self.src_language)\n",
        "            tgt_sentence = self.tokenize_seq_and_add_special_symbols(tgt_sentence, self.tgt_language)\n",
        "            if self.longer_than_max_length(src_sentence) or self.longer_than_max_length(tgt_sentence):\n",
        "                continue\n",
        "            sentences[self.src_language].append(src_sentence)\n",
        "            sentences[self.tgt_language].append(tgt_sentence)\n",
        "\n",
        "        for lang in [self.src_language, self.tgt_language]:\n",
        "            # TODO: Build vocabulary\n",
        "            vocab[lang] = build_vocab_from_iterator(iterator=, specials=, min_freq=1)\n",
        "\n",
        "            # TODO: Set default index as unknown token\n",
        "            vocab[lang].set_default_index(vocab[lang]['<>'])\n",
        "            vocab_size[lang] = len(vocab[lang])\n",
        "\n",
        "        return vocab, vocab_size\n",
        "\n",
        "\n",
        "    def make_dataloader(self, text_data: dataset , batch_size: int) -> DataLoader:\n",
        "        src_batch, tgt_batch = [], []\n",
        "\n",
        "        # Text data is tokenized and padded\n",
        "        for src_sentence, tgt_sentence in text_data:\n",
        "            src_sentence = self.tokenize_seq_and_add_special_symbols(src_sentence, src_language)\n",
        "            tgt_sentence = self.tokenize_seq_and_add_special_symbols(tgt_sentence, tgt_language)\n",
        "            if self.longer_than_max_length(src_sentence) or self.longer_than_max_length(tgt_sentence):\n",
        "                continue\n",
        "            src_sentence = self.transform_tokens_to_tensor(src_sentence, src_language)\n",
        "            tgt_sentence = self.transform_tokens_to_tensor(tgt_sentence, tgt_language)\n",
        "\n",
        "            # Padding\n",
        "            src_pad_tensor, tgt_pad_tensor = (torch.full((max_length,), pad_idx) for _ in range(2))\n",
        "            src_pad_tensor[:len(src_sentence)] = src_sentence\n",
        "            tgt_pad_tensor[:len(tgt_sentence)] = tgt_sentence\n",
        "            src_batch.append(src_pad_tensor)\n",
        "            tgt_batch.append(tgt_pad_tensor)\n",
        "\n",
        "        # TODO: Convert to tensor for creating dataset\n",
        "        src_batch_tensor =\n",
        "        tgt_batch_tensor =\n",
        "        dataset = TensorDataset(src_batch_tensor, tgt_batch_tensor)\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4)\n",
        "\n",
        "        return dataloader"
      ],
      "metadata": {
        "id": "1EBchGDbnBIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-3. Train"
      ],
      "metadata": {
        "id": "d2nsCaoGY_T8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train 코드입니다. TODO 부분을 채워주세요.\n",
        "지난번과 마찬가지로 training 관련 코드는 자유롭게 수정하셔도 됩니다. 원하시면 learning rate scheduler를 사용하셔도 되고 다른 training technique을 사용하셔도 됩니다. 대신 마지막 코드에서 test set에 대한 evaluation 결과 **BLEU score가 0.2 이상 나와야 됩니다.**"
      ],
      "metadata": {
        "id": "bn0xX3smZG6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch_for_train(model: nn.Module,\n",
        "                optimizer: torch.optim.Optimizer,\n",
        "                criterion: nn.Module,\n",
        "                batch: Tensor,\n",
        "                ) -> float:\n",
        "\n",
        "    src_batch, tgt_batch = batch\n",
        "    src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    src_padding_mask = create_padding_mask_for_attention(src_batch, pad_idx)\n",
        "    tgt_padding_mask = create_look_ahead_mask_for_attention(tgt_batch, pad_idx)\n",
        "\n",
        "    # TODO: Output 생성\n",
        "    decoder_output =\n",
        "\n",
        "    # Process output and calculate loss\n",
        "    loss = calculate_loss(decoder_output, tgt_batch, criterion)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def train(train_dataloader: DataLoader,\n",
        "          model: nn.Module,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          ) -> float:\n",
        "\n",
        "    total_loss = 0\n",
        "    num_batches = len(train_dataloader)\n",
        "    for batch in train_dataloader:\n",
        "        loss = process_batch_for_train(model, optimizer, criterion, batch)\n",
        "        total_loss += loss\n",
        "\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "dd4lCZU0nCS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training을 위해서는 아래와 같은 함수들이 추가적으로 필요합니다!\n",
        "Batch 내의 sequence들은 길이가 다를 수 있습니다. 이 때, 길이를 통일하기 위해 padding token(pad_idx)이 사용됩니다. 이 함수는 padding token이 loss 계산에 영향을 미치지 않도록 masking을 생성합니다. 즉, 실제 데이터만을 이용하여 모델의 성능을 평가하게 해줍니다.\n",
        "Padding token은 실제 데이터가 아니기 때문에, 이를 포함하여 loss를 계산하면 모델 학습이 왜곡될 수 있습니다. 정확한 backpropagation을 위해 실제 입력 데이터에 대해서만 loss를 계산해야 합니다.\n",
        "\n",
        "**Note: Loss를 계산할 때 padding token이 영향을 미치지 않도록 하는 훨씬 간단한 방법이 있는건 알지만, 여러분의 공부를 위해서 아래와 같은 함수를 구현하도록 직접 만들었으니 열심히 해주세요ㅎㅎ**"
      ],
      "metadata": {
        "id": "INUcUdcdZlRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask_for_loss(batch: Tensor, pad_idx: int) -> Tensor:\n",
        "    \"\"\"\n",
        "    Creates a padding mask for a batch of sequences, which can be used to mask the loss calculation,\n",
        "    ignoring the padding tokens. This function is useful in training sequences where different inputs\n",
        "    have varying lengths and are padded to reach a uniform length.\n",
        "\n",
        "    The function generates a mask tensor from the input batch tensor, where each element is True if it\n",
        "    is not a padding token (indicated by pad_idx) and False if it is a padding token.\n",
        "\n",
        "    Parameters:\n",
        "        batch (Tensor): The input batch tensor containing token indices.\n",
        "                        This tensor should be of shape (batch_size, max_length).\n",
        "        pad_idx (int): The index used for padding tokens in the batch.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: A boolean tensor of the same shape as the input batch - (batch_size, max_length).\n",
        "                Each element is True if the corresponding element in the batch is not a padding token, False otherwise.\n",
        "                The tensor is moved to the device the input batch tensor is on.\n",
        "\n",
        "    Example:\n",
        "        >>> batch = torch.tensor([[1, 2, 3, 0], [4, 5, 0, 0]])\n",
        "        >>> pad_idx = 0\n",
        "        >>> mask = create_padding_mask_for_loss(batch, pad_idx)\n",
        "        >>> print(mask)\n",
        "        tensor([[ True,  True,  True, False],\n",
        "                [ True,  True, False, False]])\n",
        "    \"\"\"\n",
        "    padding_mask = (batch != pad_idx)\n",
        "\n",
        "    return padding_mask.to(device)\n",
        "\n",
        "\n",
        "def calculate_loss(decoder_output: Tensor,\n",
        "                   tgt_batch: Tensor,\n",
        "                   criterion: nn.Module,\n",
        "                   ) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the loss for a batch of predictions from a decoder model, applying a mask to exclude\n",
        "    the loss contribution from padding tokens. This function is specifically designed for sequence-to-sequence\n",
        "    models where the alignment between the decoder outputs and the target sequences needs careful handling,\n",
        "    especially around special tokens like <sos> and padding indices.\n",
        "\n",
        "    This function removes the <sos> token from the target batch before reshaping both the decoder outputs\n",
        "    and targets for loss computation, then applies a padding mask to ensure that padding tokens do not\n",
        "    contribute to the loss calculation.\n",
        "\n",
        "    Parameters:\n",
        "        decoder_output (Tensor): The output from the decoder model of shape (batch_size, max_length, vocab_size),\n",
        "                                 containing logits or probabilities for each token in the vocabulary.\n",
        "        tgt_batch (Tensor): The target sequences for the batch of shape (batch_size, max_length), containing\n",
        "                            actual token indices that the model should predict.\n",
        "        criterion (nn.Module): The loss function (e.g., cross-entropy loss) used to compute the loss between\n",
        "                               the decoder outputs and the targets.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed loss, averaged over all non-padded tokens in the batch.\n",
        "\n",
        "    Note:\n",
        "        The function assumes that 'vocab_size', 'tgt_language', and 'pad_idx' are predefined and accessible within\n",
        "        the function scope or are global variables.\n",
        "\n",
        "    Example:\n",
        "        >>> decoder_output = torch.rand(3, 5, 100)  # 3 batches, sequence length 5, vocabulary size 100\n",
        "        >>> tgt_batch = torch.tensor([[2, 5, 3, 0, 0], [1, 4, 2, 3, 0], [3, 3, 4, 2, 1]])\n",
        "        >>> criterion = nn.CrossEntropyLoss(reduction='none')  # Criterion without automatic reduction\n",
        "        >>> loss = calculate_loss(decoder_output, tgt_batch, criterion)\n",
        "        >>> print(loss)\n",
        "    \"\"\"\n",
        "    # Remove the last column of decoder_output to exclude the <sos> token from the target batch\n",
        "    decoder_output = decoder_output[:, :-1].reshape(-1, vocab_size[tgt_language])\n",
        "\n",
        "    # TODO: Remove the first column from tgt_batch, shifting to exclude <sos> token and flatten the array for loss calculation\n",
        "    target =\n",
        "\n",
        "    # Create a padding mask for the target batch to identify and ignore the padded values during loss computation\n",
        "    tgt_padding_mask = create_padding_mask_for_loss(tgt_batch, pad_idx)\n",
        "\n",
        "    # TODO: Adjust the padding mask to align with the shifted target and flatten it\n",
        "    tgt_padding_mask = tgt_padding_mask[:, 1:].reshape(-1)\n",
        "\n",
        "    # Calculate the raw loss values across all predictions and actual targets\n",
        "    loss = criterion(decoder_output, target)\n",
        "\n",
        "    # TODO: Apply the padding mask to the loss, summing only the losses of non-padded tokens, and then normalize by the number of non-padded tokens\n",
        "    loss = torch.sum() / torch.sum()\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "7X_WAC74nEbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 hyperparameter는 자유롭게 수정하셔도 되고, 추가적인 training technique을 위해 새롭게 변수를 정의하셔도 됩니다.\n",
        "\n",
        "**language_pair, special_symbols, tokenizer 등 데이터셋과 관련된 부분만 수정하지 말아주세요.**"
      ],
      "metadata": {
        "id": "AGekVytodWDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 50   # 수정하셔도 상관 없는데 20 이하로는 설정하지 마세요~\n",
        "embedding_dim =\n",
        "feed_forward_dim =\n",
        "num_layers =\n",
        "num_heads =\n",
        "batch_size =\n",
        "drop_prob =\n",
        "learning_rate =\n",
        "epochs =\n",
        "\n",
        "#####################################################\n",
        "##################### 수정 금지 #####################\n",
        "language_pair = (src_language, tgt_language)\n",
        "special_symbols = ['<unk>', '<sos>', '<eos>', '<pad>']\n",
        "tokenizer = {src_language : get_tokenizer('spacy', language='de_core_news_sm'),\n",
        "              tgt_language : get_tokenizer('spacy', language='en_core_web_sm')}\n",
        "custom_dataloader = CustomDataLoader(src_language, tgt_language, max_length, special_symbols, tokenizer)\n",
        "\n",
        "# Multi30k test set has encoding problem, so we use train and valid set for training and testing\n",
        "train_data, test_data = Multi30k(split=('train', 'valid'), language_pair=language_pair)\n",
        "vocab, vocab_size = custom_dataloader.build_vocab(text_data=train_data)\n",
        "\n",
        "train_dataloader = custom_dataloader.make_dataloader(train_data, batch_size)\n",
        "##################### 수정 금지 #####################\n",
        "#####################################################\n",
        "\n",
        "\n",
        "transformer = Transformer(vocab_size=vocab_size,\n",
        "                          sequence_length=max_length,\n",
        "                          embedding_dim=embedding_dim,\n",
        "                          feed_forward_dim=feed_forward_dim,\n",
        "                          num_heads=num_heads,\n",
        "                          num_layers=num_layers,\n",
        "                          drop_prob=drop_prob).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(reduction='none')     # 수정하셔도 되는데 그대로 사용하시는 것을 권장합니다.\n",
        "transformer_optimizer = torch.optim.Adam(transformer.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "7cKw-Mt4nLA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    train_loss = train(train_dataloader, transformer, transformer_optimizer, criterion)\n",
        "\n",
        "    print('epoch:', '%04d' % (epoch + 1), 'cost =', '{:.6f}'.format(train_loss))"
      ],
      "metadata": {
        "id": "jg5wxWyAPIWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-4. Evaluation\n",
        "\n",
        "Evaluation을 위한 코드입니다. 그대로 사용해주세요."
      ],
      "metadata": {
        "id": "rQCelVBHdjcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch_for_test(model: nn.Module,\n",
        "               criterion: nn.Module,\n",
        "               batch: Tensor,\n",
        "               ) -> float:\n",
        "\n",
        "    src_batch, tgt_batch = batch\n",
        "    src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
        "\n",
        "    src_padding_mask = create_padding_mask_for_attention(src_batch, pad_idx)\n",
        "    tgt_padding_mask = create_look_ahead_mask_for_attention(tgt_batch, pad_idx)\n",
        "    decoder_output = model(src_batch, tgt_batch, src_padding_mask, tgt_padding_mask)\n",
        "\n",
        "    bleu_score = 0\n",
        "    for i in range(batch_size):\n",
        "        bleu_score += print_translation_and_calculate_bleu_score(src_batch[i], tgt_batch[i], decoder_output[i])\n",
        "\n",
        "    return bleu_score / batch_size\n",
        "\n",
        "\n",
        "def test(test_dataloader: DataLoader,\n",
        "          model: nn.Module,\n",
        "          criterion: nn.Module,\n",
        "          ) -> float:\n",
        "\n",
        "    bleu_score_total = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            bleu_score = process_batch_for_test(model, criterion, batch)\n",
        "            bleu_score_total.append(bleu_score)\n",
        "\n",
        "    print(f\"BLEU Score: {sum(bleu_score_total) / len(bleu_score_total)}\")"
      ],
      "metadata": {
        "id": "P4o5-M3rnFmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3-4-1. BLEU Score: 기계 번역의 성능 평가 도구(evaluation metric)\n",
        "\n",
        "#### **개요**\n",
        "![](https://github.com/hyunwoongko/transformer/raw/master/image/transformer-model-size.jpg)\n",
        "\n",
        "BLEU(Bilingual Evaluation Understudy) Score는 기계 번역의 정확성을 평가하는 데 널리 사용되는 지표입니다. 2002년에 Kishore Papineni와 그의 동료들에 의해 개발되었습니다. 이 점수는 기계 번역 결과와 하나 이상의 참조 번역 사이의 유사성을 측정함으로써 번역의 품질을 수치적으로 평가합니다.\n",
        "\n",
        "#### **계산 방법**\n",
        "BLEU 점수는 기계 번역된 텍스트와 사람이 번역한 참조 텍스트 사이의 단어 및 구(phrase)의 일치도를 측정합니다. 주요 계산 요소는 다음과 같습니다:\n",
        "\n",
        "1. **n-gram 일치**: BLEU 점수는 1-gram에서 4-gram까지의 n-gram 일치를 측정합니다. n-gram은 연속된 n개의 단어로 구성된 문자열입니다. 기계 번역의 n-gram이 참조 번역에 얼마나 자주 등장하는지를 분석하여 일치율을 계산합니다.\n",
        "\n",
        "2. **정밀도(precision)의 조정**: 단순한 정밀도 측정은 기계 번역에서 반복되는 구문을 과대평가할 수 있습니다. BLEU는 수정된 정밀도(modified precision)를 사용하여 이를 조정합니다. 각 n-gram의 빈도는 참조 번역에서의 최대 빈도에 맞춰 제한됩니다.\n",
        "\n",
        "3. **문장 길이의 조정**: 번역된 문장이 너무 짧을 경우, BLEU 점수는 브레버리 페널티(brevity penalty)를 적용하여 점수를 감소시킵니다. 이는 번역의 완성도를 보장하기 위해 설계되었습니다.\n",
        "\n",
        "#### **사용 및 한계**\n",
        "BLEU 점수는 대규모 평가에서 빠르고 일관된 평가를 제공합니다. 그러나 모든 언어적 미묘함이나 문맥을 완전히 포착하지는 못한다는 비판도 있습니다. 예를 들어, 문법적으로 올바르거나 의미상 적합한 번역이 낮은 BLEU 점수를 받을 수 있습니다.\n",
        "\n",
        "#### **결론**\n",
        "기계 번역 품질을 평가하는 도구로서, BLEU 점수는 여전히 중요한 역할을 합니다. 그러나 최적의 평가를 위해 다른 메트릭스와 함께 사용되어야 합니다. BLEU는 번역의 높은 정확도와 유창성을 보장하기 위한 초기 단계의 평가 도구로서 활용될 수 있습니다.\n",
        "\n",
        "BLEU score에 대해 더 자세히 알고 싶으시면 아래 링크에 있는 자료를 봐주세요:\n",
        "\n",
        "https://wikidocs.net/31695"
      ],
      "metadata": {
        "id": "8akPDfCqdnh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer를 통해 번역된 문장을 실제로 출력하고, BLEU score도 계산하는 코드입니다. 그대로 사용해주세요."
      ],
      "metadata": {
        "id": "RjG3fLTIeqFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def get_text(sequence, language):\n",
        "    # Truncate sequence before <eos>\n",
        "    eos_index = (sequence == eos_idx).nonzero(as_tuple=True)\n",
        "    if eos_index[0].numel() > 0:\n",
        "        sequence = sequence[:eos_index[0][0].item()]\n",
        "    return ' '.join([vocab[language].get_itos()[idx] for idx in sequence])\n",
        "\n",
        "def print_translation_and_calculate_bleu_score(src_sequence: Tensor, tgt_sequence: Tensor, output_sequence: Tensor):\n",
        "    output_sequence = output_sequence.max(dim=-1)[1]\n",
        "    sequences = {'Source': (src_sequence[1:], src_language), 'Target': (tgt_sequence[1:], tgt_language), 'Output': (output_sequence, tgt_language)}\n",
        "\n",
        "    reference = []\n",
        "    for domain, (sequence, language) in sequences.items():\n",
        "        text = get_text(sequence, language)\n",
        "        print(f\"{domain}: {text}\")\n",
        "\n",
        "        if domain != 'Source':\n",
        "            words = text.split()\n",
        "            if domain == 'Target':\n",
        "                reference.append(words)\n",
        "            else:  # Output\n",
        "                score = sentence_bleu(reference, words, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "    print(\"\\n\")\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "4qUircLHnG2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "아래 코드를 실행하면 다음과 같은 구조의 출력을 보게 됩니다:\n",
        "\n",
        "```\n",
        "Source:\n",
        "Target:\n",
        "Output:\n",
        "```\n",
        "여기서 source는 우리가 번역하고자 하는 문장(독일어), target은 우리가 원하는 번역된 문장(영어), output은 Transformer 모델이 생성한 문장(영어) 입니다.\n",
        "각각의 data sample에 대해 위와 같은 결과가 출력되고 맨 아래에 계산된 BLUE score를 보실 수 있습니다.\n",
        "\n",
        " **0.2 넘기셔야 됩니다!**"
      ],
      "metadata": {
        "id": "ltYpaMGyez2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = custom_dataloader.make_dataloader(test_data, batch_size)\n",
        "test(test_dataloader, transformer, criterion)"
      ],
      "metadata": {
        "id": "jrRdOFiukpJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "과제를 수행하시느라 정말 수고 많으셨습니다😍! 지난주와 이번주에 걸쳐 Transformer 전체를 구현하고, 그 모델을 사용하여 텍스트 분류(Text Classification)와 기계 번역(Machine Translation)의 두 가지 작업을 수행하게 되었습니다. 이 내용을 제대로 이해하시면 향후 코딩할 때 큰 도움이 될 것입니다. 궁금한 점이 있으시다면 언제든지 편하게 질문해 주세요. 다시 한번 고생하셨습니다!"
      ],
      "metadata": {
        "id": "Ncbyhbc7g6vO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}